{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在线加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 828/828 [00:00<00:00, 415kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 156M/156M [00:17<00:00, 8.75MB/s] \n",
      "Some weights of the model checkpoint at hfl/rbt3 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"hfl/rbt3\", force_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型下载 + 离线加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone \"https://huggingface.co/hfl/rbt3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "Cloning into 'rbt3'...\n"
     ]
    }
   ],
   "source": [
    "# 只下载.bin结尾的文件\n",
    "!git lfs clone \"https://huggingface.co/hfl/rbt3\" --include=\"*.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rbt3 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"rbt3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"./rbt3/\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.22.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型相关参数\n",
    "# model.config\n",
    "config = AutoConfig.from_pretrained(\"./rbt3/\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 是否输出attention的结果\n",
    "config.output_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8440, 8129, 4500,  784,  720, 3227, 1305, 8043,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"910用什么显卡？\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rbt3\")\n",
    "inputs = tokenizer(sen, return_tensors = \"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无Model Head的模型调用\n",
    "\n",
    "仅用模型进行输入编码，得到last hiddenstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rbt3 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"rbt3\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2689,  0.7226,  0.9236,  ..., -0.1531, -0.1403, -0.4130],\n",
       "         [ 0.0633, -0.2281,  0.0851,  ..., -0.0593, -0.0040,  0.0729],\n",
       "         [-0.0524, -0.0533, -0.0146,  ..., -0.2434,  0.1318, -0.4035],\n",
       "         ...,\n",
       "         [ 0.7591, -0.6243,  0.5458,  ..., -0.6197, -0.1946, -0.3970],\n",
       "         [ 0.4196,  0.1885, -0.3636,  ..., -1.0681,  0.3478, -0.1917],\n",
       "         [ 0.2623,  0.7232,  0.9190,  ..., -0.1471, -0.1434, -0.4107]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.5735e-01, -9.9937e-01, -9.9997e-01, -7.9634e-01,  2.3999e-01,\n",
       "         -5.3624e-01, -4.2660e-01, -3.5820e-01,  8.0749e-01,  9.9760e-01,\n",
       "          8.8484e-02, -1.0000e+00, -2.0974e-01,  9.9984e-01, -9.9997e-01,\n",
       "          9.9999e-01,  9.9861e-01,  9.9561e-01, -9.8978e-01,  2.4521e-01,\n",
       "         -9.9852e-01,  3.4638e-01,  9.1183e-02,  9.9103e-01,  9.9943e-01,\n",
       "         -9.9901e-01, -9.9582e-01, -7.8239e-01, -9.9378e-01, -9.8170e-01,\n",
       "         -5.6116e-01, -9.9999e-01,  1.2910e-01, -2.0641e-01,  7.6147e-01,\n",
       "          8.7121e-01,  2.1545e-01,  8.5164e-01, -9.0389e-01, -9.9654e-01,\n",
       "          8.4644e-02,  9.9861e-01,  1.2550e-01,  9.9755e-01,  4.0142e-01,\n",
       "          1.0136e-01,  9.9872e-01,  8.5912e-01, -2.3945e-01, -9.8623e-01,\n",
       "          2.7060e-01,  3.2203e-01, -1.3405e-01,  9.7680e-01,  4.6605e-01,\n",
       "          3.8186e-03, -4.4588e-01, -9.9995e-01, -9.9967e-01,  7.5771e-01,\n",
       "         -9.9653e-01,  9.9988e-01,  9.8833e-01,  9.9994e-01,  2.6414e-01,\n",
       "          6.2143e-02,  9.9570e-01, -5.0900e-01, -4.6069e-01, -9.9993e-01,\n",
       "         -9.8233e-01, -7.9209e-01, -9.8925e-01, -2.5482e-01, -2.0117e-01,\n",
       "         -9.9796e-01,  9.9758e-01, -1.4412e-01,  9.9990e-01,  1.3003e-01,\n",
       "         -9.9045e-01, -4.6168e-01, -9.4513e-02,  3.7165e-01,  9.9703e-01,\n",
       "          9.9975e-01,  1.8027e-01,  7.3584e-01, -3.2670e-01, -9.9785e-01,\n",
       "         -2.5515e-01,  9.9914e-01,  9.9988e-01, -9.9763e-01,  9.9992e-01,\n",
       "         -1.1338e-01,  4.4986e-01,  1.5285e-01, -9.9312e-01, -5.0072e-01,\n",
       "         -3.1171e-01,  5.6062e-02,  9.9995e-01,  9.8369e-01,  9.9667e-01,\n",
       "         -9.9923e-01, -9.3381e-01,  9.8925e-01, -9.6220e-01, -6.1786e-01,\n",
       "          1.0000e+00, -9.8875e-01,  9.9987e-01,  9.9991e-01,  9.9685e-01,\n",
       "         -9.9953e-01, -5.9866e-01, -3.6638e-01, -9.9949e-01,  9.9998e-01,\n",
       "         -9.7146e-01,  2.5014e-01, -9.9013e-01, -6.0165e-02, -3.3706e-01,\n",
       "         -9.9966e-01, -9.6620e-02,  4.0875e-01, -5.6157e-01, -9.9983e-01,\n",
       "         -9.7662e-01, -9.9795e-01,  8.6359e-01,  9.9797e-01, -9.6097e-01,\n",
       "          1.5190e-01,  4.1293e-01,  2.0873e-01, -9.9962e-01, -4.0945e-01,\n",
       "         -9.9994e-01,  9.6967e-01,  3.7515e-01,  9.5929e-01, -1.4378e-02,\n",
       "          9.9554e-01, -9.9708e-01,  9.9941e-01,  9.9994e-01, -6.9936e-01,\n",
       "         -6.7630e-01,  3.3511e-01, -9.7031e-01,  5.9530e-01, -2.7054e-01,\n",
       "         -1.9906e-01,  9.9922e-01,  4.3229e-02, -8.8739e-01,  1.0000e+00,\n",
       "         -9.9996e-01, -5.8088e-01,  7.4017e-01,  9.8739e-01,  9.9999e-01,\n",
       "         -9.9998e-01,  2.6708e-01, -1.0000e+00,  9.7226e-01,  2.9365e-02,\n",
       "          9.9805e-01,  9.8975e-01,  6.7274e-01,  9.9929e-01, -8.8839e-01,\n",
       "         -9.9991e-01,  1.8676e-01, -9.9969e-01, -2.3714e-01,  9.9414e-01,\n",
       "          1.1997e-01,  6.9724e-01,  9.9988e-01,  9.3137e-01,  3.6810e-01,\n",
       "         -3.0795e-02, -3.1307e-01, -8.8759e-01,  2.1598e-01,  9.7206e-01,\n",
       "          9.7353e-01, -8.9080e-01, -2.4082e-01,  9.7266e-01, -1.5479e-01,\n",
       "         -2.8500e-01, -9.9073e-01, -9.9961e-01,  9.9954e-01,  7.6660e-01,\n",
       "          7.6796e-01, -2.2721e-02,  9.9989e-01,  8.8956e-01,  9.9931e-01,\n",
       "         -6.3884e-01,  9.9083e-01,  1.8677e-01,  4.6530e-01,  4.2715e-01,\n",
       "          7.6410e-02,  3.3130e-01,  9.9999e-01, -8.9217e-01, -9.9850e-01,\n",
       "         -1.2107e-01, -9.9821e-01, -4.0196e-01, -9.9082e-01,  3.6659e-02,\n",
       "         -2.9748e-01,  9.9416e-01, -4.8953e-01, -4.7619e-01,  9.9923e-01,\n",
       "         -9.9996e-01,  2.9886e-02, -1.0000e+00, -9.9976e-01,  9.9896e-01,\n",
       "         -1.5141e-01, -9.9999e-01,  7.8417e-01,  9.9999e-01, -9.9134e-01,\n",
       "          9.9990e-01,  1.2076e-01, -9.9908e-01, -8.2707e-02, -3.3591e-01,\n",
       "         -1.0000e+00, -9.9941e-01, -9.9974e-01,  7.8262e-01,  5.0424e-01,\n",
       "          9.9738e-01, -1.0000e+00, -6.2755e-01, -9.9930e-01,  9.9957e-01,\n",
       "          1.0000e+00,  4.6490e-01,  2.1274e-01,  1.0000e+00,  9.4760e-01,\n",
       "          7.2469e-02,  1.9381e-01, -1.9700e-01,  8.5976e-02, -9.8336e-01,\n",
       "         -4.7999e-01,  8.3512e-01, -9.9990e-01, -2.2376e-01,  3.3388e-01,\n",
       "         -9.9842e-01,  8.9099e-02,  9.5867e-01, -6.2761e-01,  2.3265e-01,\n",
       "          1.8677e-01,  5.7585e-01,  9.3757e-01, -7.0471e-01,  9.9859e-01,\n",
       "          9.4628e-01,  9.9998e-01,  9.9868e-01, -6.9158e-01, -9.6135e-01,\n",
       "          9.9114e-01,  2.3670e-01, -9.8022e-01, -6.0682e-01, -9.9289e-01,\n",
       "          9.9888e-01, -5.4259e-01,  9.9999e-01, -9.9987e-01,  9.9986e-01,\n",
       "         -8.1999e-01,  1.7793e-01,  2.3434e-01, -3.0095e-01, -8.4095e-02,\n",
       "         -3.2942e-02,  5.3671e-01, -3.0552e-01,  8.8629e-01,  9.8050e-01,\n",
       "         -7.1581e-02, -1.6856e-01,  8.5399e-01,  1.1359e-01,  8.6402e-01,\n",
       "         -9.9787e-01,  8.3321e-01,  2.6154e-01,  2.4074e-01,  1.0887e-01,\n",
       "         -9.9987e-01,  9.9922e-01, -9.9421e-01,  2.0809e-01, -9.9984e-01,\n",
       "         -9.9807e-01, -2.9455e-01, -9.6734e-02,  9.8460e-01,  9.7978e-01,\n",
       "          5.4610e-01,  5.3537e-01, -8.9958e-01, -9.6652e-01,  9.8510e-01,\n",
       "          9.2103e-01, -9.9998e-01,  7.8169e-01,  9.9355e-01, -8.1425e-01,\n",
       "         -1.1871e-01,  8.5798e-01, -5.9634e-01,  9.8883e-01, -9.8097e-01,\n",
       "         -8.6220e-02,  9.3180e-01, -5.8886e-02, -8.6753e-01,  9.9570e-01,\n",
       "         -2.9335e-01, -9.3919e-01, -3.0586e-01,  9.9905e-01, -5.8533e-01,\n",
       "          5.0335e-01, -9.9999e-01, -9.5562e-01, -8.8528e-01, -1.0862e-01,\n",
       "          5.2289e-01, -9.9998e-01,  2.5278e-01,  9.9442e-01,  9.9790e-01,\n",
       "          1.1200e-01, -2.9060e-01, -5.1565e-01, -4.0191e-01, -9.9146e-01,\n",
       "         -9.9962e-01, -9.9326e-01,  9.9999e-01,  3.3963e-01,  2.3306e-01,\n",
       "          9.9772e-01,  9.9842e-01,  9.9967e-01, -9.8469e-01,  9.9906e-01,\n",
       "          9.9574e-01,  2.1714e-01, -5.7276e-02, -8.4662e-01, -1.6914e-03,\n",
       "          6.4326e-01, -9.9684e-01,  2.9252e-01,  9.9824e-01, -2.5099e-01,\n",
       "         -8.3846e-01,  9.9489e-01,  9.9976e-01,  9.9173e-01, -2.9288e-01,\n",
       "         -8.9196e-01, -5.3925e-01, -6.7101e-01, -8.2270e-01,  1.9228e-01,\n",
       "          9.9636e-01, -9.9974e-01, -5.9788e-01,  9.0832e-01,  5.9387e-01,\n",
       "          9.8327e-01, -9.8963e-01, -2.0471e-01, -9.9989e-01, -9.9995e-01,\n",
       "         -1.0000e+00,  1.0000e+00, -4.5349e-02,  5.1441e-01, -9.9865e-01,\n",
       "          9.9985e-01,  6.2630e-01, -9.9878e-01,  4.1848e-02,  9.7820e-01,\n",
       "         -1.1063e-01,  9.2623e-01, -9.9992e-01, -5.7397e-01,  9.9711e-01,\n",
       "         -3.7494e-01,  9.5480e-01,  4.5264e-02, -9.8355e-01,  8.8862e-01,\n",
       "          9.8995e-01, -7.3910e-02, -9.6559e-01, -9.9804e-01, -4.0191e-01,\n",
       "         -1.9983e-01,  2.4123e-01, -4.1674e-01,  1.5937e-02,  8.7398e-01,\n",
       "         -4.4099e-01, -7.4185e-01, -8.0043e-02,  9.9999e-01,  6.0755e-01,\n",
       "         -9.9388e-01, -9.7271e-01,  3.2448e-01, -5.1200e-01,  8.0892e-01,\n",
       "         -9.9999e-01, -1.1096e-01,  2.4576e-01, -2.4216e-01, -8.4169e-01,\n",
       "         -9.7448e-01, -9.3342e-01,  1.3948e-01, -9.9964e-01, -9.7834e-01,\n",
       "         -8.1732e-03, -9.9962e-01, -9.8443e-01,  9.9907e-01, -9.9842e-01,\n",
       "          3.6085e-01,  9.0375e-01, -3.8500e-01, -9.9981e-01, -4.1232e-01,\n",
       "          9.9453e-01, -9.4820e-01, -2.2737e-01, -9.9153e-01,  2.8452e-01,\n",
       "         -5.0771e-01, -9.9997e-01,  1.1069e-01,  9.9258e-01,  9.9328e-01,\n",
       "          9.8295e-01,  4.9899e-01, -2.3869e-01,  8.6553e-01,  9.3040e-01,\n",
       "          9.9990e-01, -4.0715e-01,  1.7775e-01, -9.9989e-01,  3.7825e-01,\n",
       "          8.3359e-01, -1.9230e-01, -8.9745e-01, -9.9828e-01, -2.1810e-01,\n",
       "         -2.7321e-01,  2.0622e-01,  1.0000e+00,  4.9686e-01, -5.8955e-01,\n",
       "         -1.0000e+00,  3.1438e-01, -2.3129e-01,  1.8707e-01,  8.7390e-01,\n",
       "          4.6084e-01,  9.9997e-01, -9.9969e-01,  7.1411e-01, -9.9486e-01,\n",
       "         -9.9896e-01,  9.9999e-01, -9.9999e-01,  9.7485e-01,  9.8060e-01,\n",
       "         -7.9791e-01, -3.1792e-01,  7.9895e-01, -2.9709e-01,  5.6215e-01,\n",
       "          3.7468e-02,  9.4320e-01, -2.5037e-01, -9.9996e-01,  5.6613e-01,\n",
       "          9.5969e-01, -3.8455e-01, -8.3021e-01, -9.7566e-01,  1.4922e-01,\n",
       "          9.9900e-01, -8.6594e-01, -9.8834e-01, -4.6318e-01, -4.5962e-01,\n",
       "         -2.3960e-02,  5.2435e-01, -4.0268e-01, -1.0668e-02, -8.8842e-01,\n",
       "          3.8566e-01,  9.5609e-01, -9.7970e-01,  9.9708e-01, -2.1859e-01,\n",
       "         -1.2847e-01,  4.0719e-01,  5.4646e-01,  9.9886e-01, -9.9990e-01,\n",
       "         -9.9976e-01,  7.7895e-01, -1.8766e-01, -7.9972e-01,  9.9983e-01,\n",
       "         -6.5357e-01, -2.7842e-01,  4.3399e-01,  1.7288e-01, -1.3352e-01,\n",
       "          5.7572e-01,  1.0000e+00, -9.9772e-01,  1.0000e+00, -9.9998e-01,\n",
       "          9.9848e-01,  6.0369e-02,  9.9918e-01, -9.9994e-01, -2.5896e-01,\n",
       "          9.7852e-01, -9.6240e-01, -5.0482e-02, -9.7312e-01,  8.7831e-01,\n",
       "         -3.0046e-01,  4.7362e-02,  9.2362e-01, -8.8260e-01, -2.7729e-01,\n",
       "         -9.7946e-01, -1.1321e-01,  9.9065e-01, -9.9938e-01,  9.8464e-01,\n",
       "         -9.9984e-01, -8.7057e-02,  2.0668e-01, -9.9991e-01,  9.7656e-01,\n",
       "          9.9983e-01, -4.9958e-01,  9.5158e-01, -9.9981e-01,  2.4404e-01,\n",
       "         -5.0876e-02, -9.3773e-01,  7.1387e-01, -9.9988e-01, -6.2543e-01,\n",
       "         -9.4394e-01,  1.0298e-01, -9.9983e-01,  9.7377e-01,  9.4719e-01,\n",
       "         -4.6225e-01, -9.9820e-01, -3.7312e-01, -8.8588e-01, -9.9993e-01,\n",
       "          8.0301e-01, -9.9590e-01, -9.9858e-01, -2.4520e-01, -9.9700e-01,\n",
       "          9.6028e-01, -3.0257e-02,  9.9295e-01, -9.7480e-01,  1.5268e-01,\n",
       "         -2.0889e-01, -7.0808e-01,  1.0000e+00, -9.0187e-01, -9.9813e-01,\n",
       "         -9.1281e-01, -9.9715e-01, -8.3604e-01,  1.0000e+00, -7.9672e-01,\n",
       "          9.9981e-01,  1.0472e-01, -9.8747e-01, -6.1538e-04, -6.8408e-02,\n",
       "          9.9258e-01, -8.2167e-01,  3.8466e-01,  3.0010e-01,  4.1423e-01,\n",
       "          5.2021e-01, -9.9708e-01,  9.7996e-01, -1.1183e-01, -2.0492e-01,\n",
       "          9.0940e-01, -1.2478e-01, -9.9336e-01, -9.7023e-01,  4.7071e-01,\n",
       "         -5.9796e-01,  4.4637e-01,  2.5895e-01,  8.7380e-01,  9.9990e-01,\n",
       "         -9.9997e-01,  4.7378e-01, -1.0000e+00, -9.9956e-01, -4.3264e-01,\n",
       "          1.5276e-01,  9.9231e-01,  4.5916e-01,  6.0698e-01,  3.2986e-01,\n",
       "         -9.9848e-01,  6.3233e-01,  9.9180e-01,  9.9950e-01, -4.0968e-01,\n",
       "          1.0891e-01,  9.9364e-01,  9.6775e-01,  3.0791e-01, -9.8823e-01,\n",
       "         -9.8678e-01,  4.5049e-01, -9.9504e-01,  1.0000e+00,  6.2073e-01,\n",
       "          4.4725e-01,  2.4807e-01, -2.6388e-01, -9.9893e-01, -9.9053e-01,\n",
       "          2.8586e-01,  9.8793e-01, -9.9999e-01,  9.9988e-01, -9.2177e-01,\n",
       "          9.9280e-01,  9.9154e-01,  1.0000e+00,  1.2360e-01, -9.4786e-01,\n",
       "         -9.9904e-01, -9.3178e-01,  9.9123e-01,  9.6974e-01,  1.0000e+00,\n",
       "         -9.1043e-01, -5.3155e-01, -2.1680e-01, -9.9999e-01,  9.9969e-01,\n",
       "          8.1109e-02,  2.8103e-02, -2.9250e-01, -9.9534e-01, -9.9846e-01,\n",
       "          9.9999e-01, -9.9998e-01, -9.9989e-01, -9.9916e-01, -1.0000e+00,\n",
       "          9.9490e-01,  7.0970e-01,  9.9993e-01, -8.3802e-01, -9.9780e-01,\n",
       "         -9.9713e-01,  5.3059e-01, -9.4623e-02, -3.6296e-01,  3.4489e-01,\n",
       "         -9.9997e-01,  3.5270e-01,  1.6911e-01, -6.3019e-01, -1.9490e-01,\n",
       "          5.2104e-01,  9.9781e-01, -7.3391e-01, -9.8192e-01,  9.9392e-01,\n",
       "          8.9547e-01, -9.9799e-01, -2.8076e-01, -1.0000e+00,  9.5438e-01,\n",
       "         -9.3562e-01,  2.6418e-02, -6.0172e-01, -8.5185e-01, -2.6129e-01,\n",
       "         -1.0000e+00, -1.0000e+00,  9.2702e-01,  9.9997e-01, -2.0306e-01,\n",
       "         -9.3292e-01, -4.4680e-01, -7.0464e-01,  3.0293e-01,  9.2433e-01,\n",
       "          9.9975e-01,  9.7204e-01,  9.3461e-01,  3.4028e-01, -2.2997e-01,\n",
       "          9.8627e-01, -1.0000e+00,  7.5810e-01, -9.9924e-01,  1.0000e+00,\n",
       "         -9.9998e-01,  9.9982e-01, -7.3848e-01, -5.5500e-01, -3.5691e-01,\n",
       "          9.4847e-01, -9.9531e-01, -7.8603e-01,  9.9825e-01,  9.3236e-01,\n",
       "          1.0042e-01,  9.9993e-01, -2.3286e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=(tensor([[[[5.2248e-01, 7.8965e-04, 3.2595e-04,  ..., 1.5998e-04,\n",
       "           4.7109e-04, 4.7500e-01],\n",
       "          [4.2322e-03, 1.3689e-01, 9.2271e-02,  ..., 9.3892e-02,\n",
       "           1.9553e-01, 3.2320e-03],\n",
       "          [9.0925e-03, 1.2255e-01, 1.9131e-01,  ..., 6.1143e-02,\n",
       "           2.2003e-01, 3.6894e-03],\n",
       "          ...,\n",
       "          [3.2733e-02, 6.5523e-02, 1.1739e-01,  ..., 2.0817e-01,\n",
       "           6.2769e-02, 8.5138e-03],\n",
       "          [6.5610e-02, 5.8229e-02, 5.3731e-02,  ..., 3.3094e-02,\n",
       "           4.5139e-01, 1.7357e-02],\n",
       "          [5.1910e-01, 1.8232e-03, 1.2427e-03,  ..., 3.8906e-04,\n",
       "           1.7513e-03, 4.7393e-01]],\n",
       "\n",
       "         [[9.9198e-01, 4.6532e-05, 2.1009e-04,  ..., 1.0653e-04,\n",
       "           2.3107e-03, 4.3960e-03],\n",
       "          [6.9346e-02, 1.0825e-05, 9.3057e-01,  ..., 3.2277e-07,\n",
       "           1.1281e-05, 1.5067e-05],\n",
       "          [1.3564e-02, 9.8635e-01, 5.5334e-05,  ..., 1.7758e-07,\n",
       "           3.3984e-06, 1.6542e-05],\n",
       "          ...,\n",
       "          [8.0674e-02, 1.8081e-06, 7.1234e-06,  ..., 7.3754e-04,\n",
       "           5.6905e-02, 1.0747e-03],\n",
       "          [2.0975e-01, 2.8798e-06, 1.5670e-05,  ..., 2.1241e-02,\n",
       "           5.6645e-04, 7.6766e-01],\n",
       "          [5.6687e-01, 3.8985e-06, 4.6047e-05,  ..., 1.7894e-04,\n",
       "           4.3009e-01, 2.4177e-03]],\n",
       "\n",
       "         [[1.8556e-01, 5.4907e-02, 3.5322e-02,  ..., 3.5392e-02,\n",
       "           1.4922e-01, 3.0957e-01],\n",
       "          [6.0412e-01, 1.5774e-01, 3.6567e-02,  ..., 5.1744e-03,\n",
       "           5.4038e-02, 1.1382e-01],\n",
       "          [5.1813e-01, 1.8540e-01, 1.0871e-01,  ..., 5.3927e-03,\n",
       "           5.6006e-02, 8.8505e-02],\n",
       "          ...,\n",
       "          [2.3756e-01, 7.9247e-02, 1.1537e-01,  ..., 8.4705e-02,\n",
       "           6.0796e-02, 4.8941e-02],\n",
       "          [1.8269e-02, 3.2376e-02, 6.1298e-02,  ..., 1.3409e-02,\n",
       "           6.0132e-02, 8.8350e-02],\n",
       "          [4.4915e-02, 5.5296e-02, 5.3169e-02,  ..., 6.7056e-02,\n",
       "           2.4241e-01, 1.5640e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[6.7843e-01, 1.4700e-02, 3.2285e-02,  ..., 1.7593e-02,\n",
       "           3.0123e-02, 6.5481e-02],\n",
       "          [2.9210e-01, 1.6804e-01, 3.0545e-02,  ..., 2.1290e-02,\n",
       "           3.5538e-02, 3.0363e-01],\n",
       "          [3.4059e-01, 4.6178e-02, 6.3118e-02,  ..., 7.8814e-02,\n",
       "           5.6427e-02, 3.2819e-01],\n",
       "          ...,\n",
       "          [4.9690e-01, 7.9029e-02, 4.9632e-02,  ..., 2.4487e-01,\n",
       "           1.0934e-02, 7.1088e-02],\n",
       "          [1.0522e-01, 1.4536e-02, 3.4923e-02,  ..., 7.5150e-03,\n",
       "           3.4800e-01, 3.3792e-01],\n",
       "          [4.5734e-01, 3.5466e-02, 6.0673e-02,  ..., 3.9889e-02,\n",
       "           4.2784e-02, 1.7618e-01]],\n",
       "\n",
       "         [[9.6559e-01, 5.0962e-03, 1.3025e-02,  ..., 9.6045e-04,\n",
       "           2.0089e-03, 5.0295e-03],\n",
       "          [2.1222e-02, 1.3151e-01, 4.1268e-01,  ..., 8.9659e-05,\n",
       "           6.7544e-03, 2.3876e-02],\n",
       "          [1.0354e-02, 1.6035e-02, 2.1238e-02,  ..., 5.3639e-04,\n",
       "           4.9586e-04, 4.0830e-03],\n",
       "          ...,\n",
       "          [7.2693e-03, 2.0629e-03, 1.1274e-03,  ..., 2.4429e-02,\n",
       "           7.3421e-01, 2.2616e-01],\n",
       "          [1.1278e-02, 1.5214e-03, 1.3142e-02,  ..., 3.3216e-03,\n",
       "           1.0334e-01, 8.6110e-01],\n",
       "          [9.9445e-01, 1.2999e-04, 3.8217e-04,  ..., 5.4332e-05,\n",
       "           2.8123e-04, 4.1872e-03]],\n",
       "\n",
       "         [[4.0401e-01, 6.5300e-02, 2.9548e-02,  ..., 1.3085e-02,\n",
       "           4.0918e-02, 3.6200e-01],\n",
       "          [4.6488e-01, 2.3517e-01, 5.7457e-02,  ..., 6.3161e-02,\n",
       "           2.1512e-02, 2.3734e-02],\n",
       "          [9.0785e-02, 6.1766e-01, 2.6488e-02,  ..., 9.5565e-03,\n",
       "           5.8331e-02, 3.2307e-02],\n",
       "          ...,\n",
       "          [5.0366e-02, 9.3583e-04, 2.4741e-03,  ..., 2.9906e-02,\n",
       "           1.8222e-02, 1.4471e-02],\n",
       "          [2.6581e-01, 5.0356e-03, 1.6451e-03,  ..., 5.3035e-01,\n",
       "           8.1862e-02, 2.6140e-02],\n",
       "          [6.2663e-02, 4.9472e-03, 4.8490e-04,  ..., 2.3709e-02,\n",
       "           3.9828e-01, 4.9706e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.5504e-01, 1.5147e-02, 7.6681e-03,  ..., 1.2294e-02,\n",
       "           4.4542e-02, 4.3597e-01],\n",
       "          [4.9842e-01, 6.4503e-03, 7.0205e-03,  ..., 2.6892e-03,\n",
       "           4.8464e-04, 4.6515e-01],\n",
       "          [1.5847e-01, 6.8337e-01, 2.1836e-04,  ..., 7.1579e-05,\n",
       "           7.4133e-04, 1.5524e-01],\n",
       "          ...,\n",
       "          [9.9344e-02, 3.1024e-05, 1.5778e-04,  ..., 1.5710e-04,\n",
       "           6.2475e-03, 9.6688e-02],\n",
       "          [2.5758e-01, 3.4164e-03, 5.0917e-04,  ..., 3.6273e-01,\n",
       "           4.3844e-02, 2.6218e-01],\n",
       "          [4.5603e-01, 1.4231e-02, 7.4322e-03,  ..., 1.1731e-02,\n",
       "           4.6555e-02, 4.3673e-01]],\n",
       "\n",
       "         [[4.7337e-01, 1.3284e-02, 9.7780e-03,  ..., 1.0308e-02,\n",
       "           6.8793e-03, 4.6243e-01],\n",
       "          [3.1225e-01, 1.3520e-02, 2.6604e-02,  ..., 6.5610e-02,\n",
       "           4.8179e-02, 3.1171e-01],\n",
       "          [2.6571e-01, 2.8986e-02, 2.9313e-02,  ..., 6.1089e-02,\n",
       "           1.3614e-01, 2.6561e-01],\n",
       "          ...,\n",
       "          [4.6100e-01, 4.4635e-03, 7.3805e-03,  ..., 1.7414e-02,\n",
       "           2.2795e-02, 4.5633e-01],\n",
       "          [1.9322e-01, 3.9304e-02, 4.1305e-02,  ..., 8.9410e-02,\n",
       "           1.4741e-01, 1.9795e-01],\n",
       "          [4.7527e-01, 1.2075e-02, 8.9316e-03,  ..., 9.8120e-03,\n",
       "           6.4784e-03, 4.6459e-01]],\n",
       "\n",
       "         [[4.8618e-01, 1.4958e-02, 3.2230e-03,  ..., 6.5774e-03,\n",
       "           1.1341e-02, 4.5070e-01],\n",
       "          [4.3311e-01, 1.0278e-01, 8.1612e-03,  ..., 6.8470e-03,\n",
       "           4.7552e-03, 4.2067e-01],\n",
       "          [2.5996e-01, 1.0945e-02, 3.3784e-01,  ..., 5.8806e-02,\n",
       "           8.0016e-03, 2.5139e-01],\n",
       "          ...,\n",
       "          [1.1999e-01, 3.2257e-03, 1.5732e-03,  ..., 5.1776e-01,\n",
       "           1.9729e-03, 1.1220e-01],\n",
       "          [4.1263e-01, 6.5512e-04, 1.3977e-03,  ..., 3.5583e-03,\n",
       "           1.1928e-01, 3.8696e-01],\n",
       "          [4.8608e-01, 1.4994e-02, 3.2361e-03,  ..., 6.5293e-03,\n",
       "           1.1527e-02, 4.4989e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.5196e-01, 3.0332e-02, 2.8521e-02,  ..., 1.4787e-02,\n",
       "           1.6856e-02, 4.2900e-01],\n",
       "          [2.2092e-01, 2.7574e-02, 3.9704e-02,  ..., 6.3264e-02,\n",
       "           4.0983e-02, 2.2264e-01],\n",
       "          [1.4997e-01, 5.2666e-03, 1.7464e-02,  ..., 7.4513e-02,\n",
       "           8.3581e-02, 1.5320e-01],\n",
       "          ...,\n",
       "          [4.5008e-01, 5.5680e-03, 2.8778e-03,  ..., 2.2578e-02,\n",
       "           4.2477e-02, 4.6758e-01],\n",
       "          [3.2430e-01, 2.0116e-01, 5.0160e-02,  ..., 1.6599e-02,\n",
       "           4.2653e-02, 3.3026e-01],\n",
       "          [4.5448e-01, 2.7917e-02, 2.7557e-02,  ..., 1.4086e-02,\n",
       "           1.6337e-02, 4.3187e-01]],\n",
       "\n",
       "         [[4.9712e-01, 6.8630e-03, 3.2655e-03,  ..., 2.7017e-03,\n",
       "           7.9558e-03, 4.6670e-01],\n",
       "          [2.8390e-01, 3.0293e-02, 3.3491e-02,  ..., 8.0024e-02,\n",
       "           1.4691e-02, 2.7896e-01],\n",
       "          [1.1735e-01, 1.3213e-01, 7.9580e-02,  ..., 1.2299e-01,\n",
       "           5.5793e-02, 1.1666e-01],\n",
       "          ...,\n",
       "          [2.4401e-01, 1.0305e-01, 6.8271e-02,  ..., 3.2519e-02,\n",
       "           3.4081e-02, 2.4417e-01],\n",
       "          [1.5368e-01, 2.5319e-01, 1.5175e-01,  ..., 2.3834e-02,\n",
       "           3.5133e-02, 1.5463e-01],\n",
       "          [4.9657e-01, 7.2477e-03, 3.2682e-03,  ..., 2.7579e-03,\n",
       "           8.0043e-03, 4.6668e-01]],\n",
       "\n",
       "         [[4.5992e-01, 4.0478e-02, 6.7612e-03,  ..., 1.5670e-02,\n",
       "           7.7045e-03, 4.4476e-01],\n",
       "          [3.3106e-01, 2.4669e-02, 3.1903e-02,  ..., 9.2963e-02,\n",
       "           8.6170e-03, 3.2072e-01],\n",
       "          [2.0098e-01, 5.0764e-02, 5.0616e-02,  ..., 1.3720e-01,\n",
       "           1.1272e-02, 1.9329e-01],\n",
       "          ...,\n",
       "          [1.4046e-01, 2.7373e-02, 1.8383e-02,  ..., 2.7284e-01,\n",
       "           1.7017e-03, 1.3775e-01],\n",
       "          [4.0968e-01, 1.7075e-02, 2.0835e-02,  ..., 1.9612e-02,\n",
       "           8.2899e-03, 4.0306e-01],\n",
       "          [4.5817e-01, 4.1944e-02, 6.7976e-03,  ..., 1.6901e-02,\n",
       "           7.7276e-03, 4.4327e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.3117, 0.0241, 0.0244,  ..., 0.0215, 0.0825, 0.3064],\n",
       "          [0.4745, 0.0016, 0.0019,  ..., 0.0029, 0.0213, 0.4698],\n",
       "          [0.4736, 0.0046, 0.0048,  ..., 0.0027, 0.0200, 0.4676],\n",
       "          ...,\n",
       "          [0.3547, 0.0210, 0.0280,  ..., 0.0104, 0.0063, 0.3517],\n",
       "          [0.2440, 0.0244, 0.0263,  ..., 0.0238, 0.0465, 0.2408],\n",
       "          [0.3133, 0.0241, 0.0244,  ..., 0.0214, 0.0818, 0.3080]],\n",
       "\n",
       "         [[0.0053, 0.0421, 0.0503,  ..., 0.0143, 0.2291, 0.0053],\n",
       "          [0.3402, 0.0347, 0.2437,  ..., 0.0069, 0.0059, 0.3355],\n",
       "          [0.2157, 0.2133, 0.0121,  ..., 0.0300, 0.0091, 0.2131],\n",
       "          ...,\n",
       "          [0.4230, 0.0038, 0.0006,  ..., 0.0097, 0.0133, 0.4179],\n",
       "          [0.0460, 0.0026, 0.0008,  ..., 0.4509, 0.0130, 0.0455],\n",
       "          [0.0053, 0.0424, 0.0506,  ..., 0.0144, 0.2294, 0.0053]],\n",
       "\n",
       "         [[0.4437, 0.0056, 0.0023,  ..., 0.0031, 0.0825, 0.4398],\n",
       "          [0.4054, 0.1008, 0.0204,  ..., 0.0109, 0.0134, 0.3982],\n",
       "          [0.4090, 0.0511, 0.0344,  ..., 0.0345, 0.0109, 0.4013],\n",
       "          ...,\n",
       "          [0.3820, 0.0329, 0.0327,  ..., 0.0735, 0.0174, 0.3741],\n",
       "          [0.3712, 0.0327, 0.0103,  ..., 0.0734, 0.0295, 0.3683],\n",
       "          [0.4439, 0.0056, 0.0023,  ..., 0.0030, 0.0822, 0.4401]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0295, 0.2822, 0.1704,  ..., 0.0552, 0.0683, 0.0292],\n",
       "          [0.2964, 0.0157, 0.0273,  ..., 0.0241, 0.0109, 0.2918],\n",
       "          [0.2434, 0.0107, 0.0165,  ..., 0.0520, 0.0191, 0.2394],\n",
       "          ...,\n",
       "          [0.4708, 0.0015, 0.0017,  ..., 0.0176, 0.0244, 0.4674],\n",
       "          [0.4383, 0.0286, 0.0188,  ..., 0.0266, 0.0306, 0.4349],\n",
       "          [0.0296, 0.2827, 0.1706,  ..., 0.0548, 0.0681, 0.0293]],\n",
       "\n",
       "         [[0.0376, 0.0665, 0.0590,  ..., 0.0774, 0.2295, 0.0373],\n",
       "          [0.4781, 0.0188, 0.0106,  ..., 0.0016, 0.0067, 0.4764],\n",
       "          [0.4609, 0.0377, 0.0120,  ..., 0.0017, 0.0041, 0.4585],\n",
       "          ...,\n",
       "          [0.1302, 0.0028, 0.0065,  ..., 0.0283, 0.0053, 0.1287],\n",
       "          [0.1126, 0.0068, 0.0085,  ..., 0.0700, 0.0338, 0.1111],\n",
       "          [0.0377, 0.0667, 0.0590,  ..., 0.0781, 0.2301, 0.0374]],\n",
       "\n",
       "         [[0.1390, 0.0698, 0.1208,  ..., 0.0633, 0.1019, 0.1359],\n",
       "          [0.2408, 0.0147, 0.4109,  ..., 0.0036, 0.0047, 0.2375],\n",
       "          [0.2899, 0.0181, 0.0286,  ..., 0.0129, 0.0021, 0.2869],\n",
       "          ...,\n",
       "          [0.4543, 0.0036, 0.0011,  ..., 0.0116, 0.0610, 0.4523],\n",
       "          [0.3351, 0.0532, 0.0260,  ..., 0.1740, 0.0267, 0.3346],\n",
       "          [0.1396, 0.0703, 0.1209,  ..., 0.0630, 0.1019, 0.1365]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)), cross_attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [batch_size, token_len, hidden_size]\n",
    "output.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带Model Head模型的调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rbt3 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at rbt3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "clz_model = AutoModelForSequenceClassification.from_pretrained(\"rbt3\", num_labels = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3884, -0.2557, -0.1954,  0.5341,  0.1535,  0.1943,  0.5151, -0.2480,\n",
       "          0.9916, -0.1675]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = clz_model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clz_model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textcls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
